import { MongoDatasetTraining } from '@fastgpt/service/core/dataset/training/schema';
import { pushQABill } from '@/service/support/wallet/bill/push';
import { DatasetDataIndexTypeEnum, TrainingModeEnum } from '@fastgpt/global/core/dataset/constant';
import { sendOneInform } from '../support/user/inform/api';
import { getAIApi } from '@fastgpt/service/core/ai/config';
import type { ChatMessageItemType } from '@fastgpt/global/core/ai/type.d';
import { addLog } from '@fastgpt/service/common/system/log';
import { splitText2Chunks } from '@fastgpt/global/common/string/textSplitter';
import { replaceVariable } from '@fastgpt/global/common/string/tools';
import { Prompt_AgentQA } from '@/global/core/prompt/agent';
import { pushDataToDatasetCollection } from '@/pages/api/core/dataset/data/pushData';
import { getErrText } from '@fastgpt/global/common/error/utils';
import { authTeamBalance } from '../support/permission/auth/bill';
import type { PushDatasetDataChunkProps } from '@fastgpt/global/core/dataset/api.d';
import { UserErrEnum } from '@fastgpt/global/common/error/code/user';
import { lockTrainingDataByTeamId } from '@fastgpt/service/core/dataset/training/controller';

const reduceQueue = (retry = false) => {
  global.qaQueueLen = global.qaQueueLen > 0 ? global.qaQueueLen - 1 : 0;
  if (global.qaQueueLen === 0 && retry) {
    setTimeout(() => {
      generateQA();
    }, 60000);
  }

  return global.vectorQueueLen === 0;
};

export async function generateQA(): Promise<any> {
  if (global.qaQueueLen >= global.systemEnv.qaMaxProcess) return;
  global.qaQueueLen++;

  // get training data
  const {
    data,
    text,
    done = false,
    error = false
  } = await (async () => {
    try {
      const data = await MongoDatasetTraining.findOneAndUpdate(
        {
          mode: TrainingModeEnum.qa,
          lockTime: { $lte: new Date(Date.now() - 6 * 60 * 1000) }
        },
        {
          lockTime: new Date()
        }
      )
        .select({
          _id: 1,
          userId: 1,
          teamId: 1,
          tmbId: 1,
          datasetId: 1,
          collectionId: 1,
          q: 1,
          model: 1,
          chunkIndex: 1,
          billId: 1,
          prompt: 1
        })
        .lean();

      // task preemption
      if (!data) {
        return {
          done: true
        };
      }
      return {
        data,
        text: data.q
      };
    } catch (error) {
      console.log(`Get Training Data error`, error);
      return {
        error: true
      };
    }
  })();

  if (done || !data) {
    if (reduceQueue()) {
      console.log(`ã€QAã€‘Task Done`);
    }
    return;
  }
  if (error) {
    reduceQueue();
    return generateQA();
  }

  // auth balance
  try {
    await authTeamBalance(data.teamId);
  } catch (error: any) {
    if (error?.statusText === UserErrEnum.balanceNotEnough) {
      // send inform and lock data
      try {
        sendOneInform({
          type: 'system',
          title: 'æ–‡æœ¬è®­ç»ƒä»»åŠ¡ä¸­æ­¢',
          content:
            'è¯¥å›¢é˜Ÿè´¦å·ä½™é¢ä¸è¶³ï¼Œæ–‡æœ¬è®­ç»ƒä»»åŠ¡ä¸­æ­¢ï¼Œé‡æ–°å……å€¼åŽå°†ä¼šç»§ç»­ã€‚æš‚åœçš„ä»»åŠ¡å°†åœ¨ 7 å¤©åŽè¢«åˆ é™¤ã€‚',
          tmbId: data.tmbId
        });
        console.log('ä½™é¢ä¸è¶³ï¼Œæš‚åœã€QAã€‘ç”Ÿæˆä»»åŠ¡');
        lockTrainingDataByTeamId(data.teamId);
      } catch (error) {}
    }

    reduceQueue();
    return generateQA();
  }

  try {
    const startTime = Date.now();
    const model = data.model ?? global.qaModels[0].model;
    const prompt = `${data.prompt || Prompt_AgentQA.description}
${replaceVariable(Prompt_AgentQA.fixedText, { text })}`;

    // request LLM to get QA
    const messages: ChatMessageItemType[] = [
      {
        role: 'user',
        content: prompt
      }
    ];

    const ai = getAIApi(undefined, 600000);
    const chatResponse = await ai.chat.completions.create({
      model,
      temperature: 0.3,
      messages,
      stream: false
    });
    console.log('ðŸš€ ~ file: generateQA.ts:138 ~ generateQA ~ chatResponse:', chatResponse);
    console.log('ðŸš€ ~ file: generateQA.ts:138 ~ generateQA ~ chatResponse:', chatResponse);
    const answer = chatResponse.choices?.[0].message?.content || '';
    const totalTokens = chatResponse.usage?.total_tokens || 0;

    const qaArr = formatSplitText(answer, text); // æ ¼å¼åŒ–åŽçš„QAå¯¹

    // get vector and insert
    await pushDataToDatasetCollection({
      teamId: data.teamId,
      tmbId: data.tmbId,
      collectionId: data.collectionId,
      data: qaArr.map((item) => ({
        ...item,
        chunkIndex: data.chunkIndex
      })),
      mode: TrainingModeEnum.chunk,
      billId: data.billId
    });

    // delete data from training
    await MongoDatasetTraining.findByIdAndDelete(data._id);

    addLog.info(`QA Training Finish`, {
      time: `${(Date.now() - startTime) / 1000}s`,
      splitLength: qaArr.length,
      usage: chatResponse.usage
    });

    // add bill
    if (qaArr.length > 0) {
      pushQABill({
        teamId: data.teamId,
        tmbId: data.tmbId,
        totalTokens,
        billId: data.billId,
        model
      });
    } else {
      addLog.info(`QA result 0:`, { answer });
    }

    reduceQueue();
    generateQA();
  } catch (err: any) {
    reduceQueue(true);
    // log
    if (err?.response) {
      addLog.info('openai error: ç”ŸæˆQAé”™è¯¯', {
        status: err.response?.status,
        stateusText: err.response?.statusText,
        data: err.response?.data
      });
    } else {
      console.log(err);
      addLog.error(getErrText(err, 'ç”Ÿæˆ QA é”™è¯¯'));
    }

    // message error or openai account error
    if (
      err?.message === 'invalid message format' ||
      err.response?.data?.error?.type === 'invalid_request_error' ||
      err?.code === 500
    ) {
      addLog.info('invalid message format', {
        text
      });
      try {
        await MongoDatasetTraining.findByIdAndUpdate(data._id, {
          lockTime: new Date('2998/5/5')
        });
      } catch (error) {}
      return generateQA();
    }

    setTimeout(() => {
      generateQA();
    }, 1000);
  }
}

/**
 * æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æŒ‰æ ¼å¼è¿”å›ž
 */
function formatSplitText(text: string, rawText: string) {
  text = text.replace(/\\n/g, '\n'); // å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
  const regex = /Q\d+:(\s*)(.*)(\s*)A\d+:(\s*)([\s\S]*?)(?=Q|$)/g; // åŒ¹é…Qå’ŒAçš„æ­£åˆ™è¡¨è¾¾å¼
  const matches = text.matchAll(regex); // èŽ·å–æ‰€æœ‰åŒ¹é…åˆ°çš„ç»“æžœ

  const result: PushDatasetDataChunkProps[] = []; // å­˜å‚¨æœ€ç»ˆçš„ç»“æžœ
  for (const match of matches) {
    const q = match[2] || '';
    const a = match[5] || '';
    if (q) {
      result.push({
        q,
        a,
        indexes: [
          {
            defaultIndex: true,
            type: DatasetDataIndexTypeEnum.qa,
            text: `${q}\n${a.trim().replace(/\n\s*/g, '\n')}`
          }
        ]
      });
    }
  }

  // empty result. direct split chunk
  if (result.length === 0) {
    const { chunks } = splitText2Chunks({ text: rawText, chunkLen: 512, countTokens: false });
    chunks.forEach((chunk) => {
      result.push({
        q: chunk,
        a: '',
        indexes: [
          {
            defaultIndex: true,
            type: DatasetDataIndexTypeEnum.chunk,
            text: chunk
          }
        ]
      });
    });
  }

  return result;
}
